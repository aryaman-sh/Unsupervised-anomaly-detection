{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c0c1008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from typing import List, Callable, Union, Any, TypeVar, Tuple\n",
    "Tensor = TypeVar('torch.tensor')\n",
    "device = torch.device(\"cuda\")\n",
    "batch_size = 64\n",
    "LR = 0.005\n",
    "weight_decay = 0.0\n",
    "scheduler_gamma = 0.95\n",
    "kld_weight = 0.00025\n",
    "max_epochs = 100\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision as vision\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b637bd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaVAE(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 latent_dim: int,\n",
    "                 hidden_dims: List = None,\n",
    "                 **kwargs) -> None:\n",
    "        super(VanillaVAE, self).__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        modules = []\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [32, 64, 128, 256, 512]\n",
    "\n",
    "        # Build Encoder\n",
    "        for h_dim in hidden_dims:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, out_channels=h_dim,\n",
    "                              kernel_size= 3, stride= 2, padding  = 1),\n",
    "                    nn.BatchNorm2d(h_dim),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "            in_channels = h_dim\n",
    "\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        self.fc_mu = nn.Linear(hidden_dims[-1]*4, latent_dim)\n",
    "        self.fc_var = nn.Linear(hidden_dims[-1]*4, latent_dim)\n",
    "\n",
    "\n",
    "        # Build Decoder\n",
    "        modules = []\n",
    "\n",
    "        self.decoder_input = nn.Linear(latent_dim, hidden_dims[-1] * 4)\n",
    "\n",
    "        hidden_dims.reverse()\n",
    "\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(hidden_dims[i],\n",
    "                                       hidden_dims[i + 1],\n",
    "                                       kernel_size=3,\n",
    "                                       stride = 2,\n",
    "                                       padding=1,\n",
    "                                       output_padding=1),\n",
    "                    nn.BatchNorm2d(hidden_dims[i + 1]),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "\n",
    "        self.final_layer = nn.Sequential(\n",
    "                            nn.ConvTranspose2d(hidden_dims[-1],\n",
    "                                               hidden_dims[-1],\n",
    "                                               kernel_size=3,\n",
    "                                               stride=2,\n",
    "                                               padding=1,\n",
    "                                               output_padding=1),\n",
    "                            nn.BatchNorm2d(hidden_dims[-1]),\n",
    "                            nn.LeakyReLU(),\n",
    "                            nn.Conv2d(hidden_dims[-1], out_channels= 1,\n",
    "                                      kernel_size= 3, padding= 1),\n",
    "                            nn.Tanh())\n",
    "\n",
    "    def encode(self, input: Tensor) -> List[Tensor]:\n",
    "        \"\"\"\n",
    "        Encodes the input by passing through the encoder network\n",
    "        and returns the latent codes.\n",
    "        :param input: (Tensor) Input tensor to encoder [N x C x H x W]\n",
    "        :return: (Tensor) List of latent codes\n",
    "        \"\"\"\n",
    "        result = self.encoder(input)\n",
    "        result = torch.flatten(result, start_dim=1)\n",
    "\n",
    "        # Split the result into mu and var components\n",
    "        # of the latent Gaussian distribution\n",
    "        mu = self.fc_mu(result)\n",
    "        log_var = self.fc_var(result)\n",
    "\n",
    "        return [mu, log_var]\n",
    "\n",
    "    def decode(self, z: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Maps the given latent codes\n",
    "        onto the image space.\n",
    "        :param z: (Tensor) [B x D]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "        result = self.decoder_input(z)\n",
    "        result = result.view(-1, 512, 2, 2)\n",
    "        result = self.decoder(result)\n",
    "        result = self.final_layer(result)\n",
    "        return result\n",
    "\n",
    "    def reparameterize(self, mu: Tensor, logvar: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Reparameterization trick to sample from N(mu, var) from\n",
    "        N(0,1).\n",
    "        :param mu: (Tensor) Mean of the latent Gaussian [B x D]\n",
    "        :param logvar: (Tensor) Standard deviation of the latent Gaussian [B x D]\n",
    "        :return: (Tensor) [B x D]\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "\n",
    "    def forward(self, input: Tensor, **kwargs) -> List[Tensor]:\n",
    "        mu, log_var = self.encode(input)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return  [self.decode(z), input, mu, log_var]\n",
    "\n",
    "    def loss_function(self,\n",
    "                      *args,\n",
    "                      **kwargs) -> dict:\n",
    "        \"\"\"\n",
    "        Computes the VAE loss function.\n",
    "        KL(N(\\mu, \\sigma), N(0, 1)) = \\log \\frac{1}{\\sigma} + \\frac{\\sigma^2 + \\mu^2}{2} - \\frac{1}{2}\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        recons = args[0]\n",
    "        input = args[1]\n",
    "        mu = args[2]\n",
    "        log_var = args[3]\n",
    "\n",
    "        kld_weight =  0.005# Account for the minibatch samples from the dataset\n",
    "        recons_loss =F.mse_loss(recons, input)\n",
    "\n",
    "\n",
    "        kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)\n",
    "\n",
    "        loss = recons_loss + kld_weight * kld_loss\n",
    "        return {'loss': loss, 'Reconstruction_Loss':recons_loss, 'KLD':-kld_loss}\n",
    "\n",
    "    def sample(self,\n",
    "               num_samples:int,\n",
    "               current_device: int, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Samples from the latent space and return the corresponding\n",
    "        image space map.\n",
    "        :param num_samples: (Int) Number of samples\n",
    "        :param current_device: (Int) Device to run the model\n",
    "        :return: (Tensor)\n",
    "        \"\"\"\n",
    "        z = torch.randn(num_samples,\n",
    "                        self.latent_dim)\n",
    "\n",
    "        z = z.to(current_device)\n",
    "\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "\n",
    "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Given an input image x, returns the reconstructed image\n",
    "        :param x: (Tensor) [B x C x H x W]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "\n",
    "        return self.forward(x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65cf96a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VanillaVAE(in_channels = 1, latent_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b17202ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.Grayscale(),\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize(\n",
    "        #    0.5, 0.5\n",
    "        #),\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset = vision.datasets.ImageFolder('/home/Student/s4606685/summer_research/oasis-3/png_data', transform=transform)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3e12991",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb76e905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = (data - torch.min(data)) / (torch.max(data)-torch.min(data))\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon, inputted, mu_x, logvar_x = model(data)\n",
    "        loss = model.loss_function(recon, inputted, mu_x, logvar_x)\n",
    "        loss = loss['loss']\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 600 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader),\n",
    "                       loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "        epoch, train_loss / len(train_loader.dataset)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "491b7ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in model.modules():\n",
    "    if isinstance(l, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "        nn.init.normal_(l.weight.data, 0.0, 0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b64c55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/106399 (0%)]\tLoss: 0.002367\n",
      "Train Epoch: 1 [38400/106399 (36%)]\tLoss: 0.000270\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 101):\n",
    "    train(epoch)\n",
    "    recon_saver(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "426ff9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_sample():\n",
    "    train_loader2 = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "    loader = enumerate(train_loader2)\n",
    "    data = next(loader)\n",
    "    data = (data - torch.min(data)) / (torch.max(data)-torch.min(data))\n",
    "    return data[1][0]\n",
    "\n",
    "def recon_saver(epoch):\n",
    "    model.eval()\n",
    "    data = get_data_sample()\n",
    "    data = data.to(device)\n",
    "    recon = model(data)\n",
    "    img_name = str(epoch) + '.png'\n",
    "    plt.imsave(img_name, recon[0][0][0].to('cpu').detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bce3e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_saver(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def6d55e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
